{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "892b81d7",
   "metadata": {},
   "source": [
    "# NLP (Natural language processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "214773c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alwinsolair/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported Modules\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import nltk\n",
    "#new line\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.stem import SnowballStemmer\n",
    "import string\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "# New module is\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline  import Pipeline, FeatureUnion, make_pipeline\n",
    "\n",
    "print(\"Imported Modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fef6ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d93a4d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cbe4b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"Jeff stole my octopus sandwich.\", \n",
    "    \"'Help!' I sobbed, sandwichlessly.\", \n",
    "    \"'Drop the sandwiches!' said the sandwich police.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29f3af1",
   "metadata": {},
   "source": [
    "##Â How do I turn a corpus of documents into a feature matrix?\n",
    "\n",
    "**Words --> numbers?????**\n",
    "\n",
    "**Corpus: list of documents**\n",
    "\n",
    " [\n",
    "     \"Jeff stole my octopus sandwich.\", \n",
    "     \"'Help!' I sobbed, sandwichlessly.\", \n",
    "     \"'Drop the sandwiches!' said the sandwich police.\"\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b7658d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_tokenizer(doc, stops=None, stemmer=None):\n",
    "    doc = word_tokenize(doc.lower())\n",
    "    tokens = [''.join([char for char in tok if char not in string.punctuation]) for tok in doc]\n",
    "    tokens = [tok for tok in tokens if tok]\n",
    "    if stops:\n",
    "        tokens = [tok for tok in tokens if (tok not in stops)]\n",
    "    if stemmer:\n",
    "        tokens = [stemmer.stem(tok) for tok in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97f98974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['jeff', 'stole', 'my', 'octopus', 'sandwich'],\n",
       " ['help', 'i', 'sobbed', 'sandwichlessly'],\n",
       " ['drop', 'the', 'sandwiches', 'said', 'the', 'sandwich', 'police']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_docs = [our_tokenizer(doc) for doc in corpus]\n",
    "tokenized_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813d9a0e",
   "metadata": {},
   "source": [
    "**Step 1: lowercase, lose punction, split into tokens**\n",
    "\n",
    "    [\n",
    "     ['jeff', 'stole', 'my', 'octopus', 'sandwich'],\n",
    "     ['help', 'i', 'sobbed', 'sandwichlessly'],\n",
    "     ['drop', 'the', 'sandwiches', 'said', 'the', 'sandwich', 'police']\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62abc677",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "457a4f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'i' in stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "596fd00e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['jeff', 'stole', 'octopus', 'sandwich'],\n",
       " ['help', 'sobbed', 'sandwichlessly'],\n",
       " ['drop', 'sandwiches', 'said', 'sandwich', 'police']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_docs = [our_tokenizer(doc, stops=stopwords) for doc in corpus]\n",
    "tokenized_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd3aa96",
   "metadata": {},
   "source": [
    "**Step 2: remove stop words**\n",
    "\n",
    "    [\n",
    "     ['jeff', 'stole', 'octopus', 'sandwich'],\n",
    "     ['help', 'sobbed', 'sandwichlessly'],\n",
    "     ['drop', 'sandwiches', 'said', 'sandwich', 'police']\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46f1cef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['jeff', 'stole', 'octopus', 'sandwich'],\n",
       " ['help', 'sob', 'sandwichless'],\n",
       " ['drop', 'sandwich', 'said', 'sandwich', 'polic']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_docs = [our_tokenizer(doc, stops=stopwords, stemmer=SnowballStemmer('english')) for doc in corpus]\n",
    "tokenized_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fee281",
   "metadata": {},
   "source": [
    "**Step 3: Stemming/Lemmatization**\n",
    "\n",
    "    [\n",
    "     ['jeff', 'stole', 'octopus', 'sandwich'],\n",
    "     ['help', 'sobbed', 'sandwichlessly'],\n",
    "     ['drop', u'sandwich', 'said', 'sandwich', 'police']\n",
    "    ]\n",
    "\n",
    "**OK now what?**\n",
    "\n",
    "Vocabulary:\n",
    "\n",
    "    ['drop', 'help', 'jeff', 'octopus', 'police', 'said', 'sandwich', 'sandwichlessly', 'sobbed', 'stole']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b09d8fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "506927e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in tokenized_docs:\n",
    "    vocab_set.update(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59fde999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['drop', 'help', 'jeff', 'octopus', 'polic', 'said', 'sandwich', 'sandwichless', 'sob', 'stole']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(vocab_set))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c544dd2",
   "metadata": {},
   "source": [
    "### Count Vectorizer, TFIDF\n",
    "\n",
    "Count vectorization\n",
    "\n",
    "***Vocabulary:***\n",
    "\n",
    "    ['drop', 'help', 'jeff', 'octopus', 'police', 'said', 'sandwich', 'sandwichlessly', 'sobbed', 'stole']\n",
    "    ['jeff', 'stole', 'octopus', 'sandwich']\n",
    "    [0, 0, 1, 1, 0, 0, 1, 0, 0, 1]\n",
    "\n",
    "    ['help', 'sobbed', 'sandwichlessly']\n",
    "    [0, 1, 0, 0, 0, 0, 0, 1, 1, 0]\n",
    "\n",
    "    ['drop', u'sandwich', 'said', 'sandwich', 'police']\n",
    "    [1, 0, 0, 0, 1, 1, 2, 0, 0, 0]\n",
    "    \n",
    "**Term frequency**\n",
    "\n",
    "$$TF_{word,document} = \\frac{\\#\\_of\\_times\\_word\\_appears\\_in\\_document}{total\\_\\#\\_of\\_words\\_in\\_document}$$\n",
    "\n",
    "    ['jeff', 'stole', 'octopus', 'sandwich']\n",
    "    [0, 0, 1/4, 1/4, 0, 0, 1/4, 0, 0, 1/4]\n",
    "\n",
    "    ['help', 'sobbed', 'sandwichlessly']\n",
    "    [0, 1/3, 0, 0, 0, 0, 0, 1/3, 1/3, 0]\n",
    "\n",
    "    ['drop', u'sandwich', 'said', 'sandwich', 'police']\n",
    "    [1/5, 0, 0, 0, 1/5, 1/5, 2/5, 0, 0, 0]\n",
    "\n",
    "### Document frequency\n",
    "\n",
    "$$ DF_{word} = \\frac{\\#\\_of\\_documents\\_containing\\_word}{total\\_\\#\\_of\\_documents} $$\n",
    "Vocabulary:\n",
    "\n",
    "    ['drop', 'help', 'jeff', 'octopus', 'police', 'said', 'sandwich', 'sandwichlessly', 'sobbed', 'stole']\n",
    "\n",
    "**Document frequency for each word:**\n",
    "\n",
    "    [1/3, 1/3, 1/3, 1/3, 1/3, 1/3, 2/3, 1/3, 1/3, 1/3]\n",
    "\n",
    "### Inverse document frequency\n",
    "\n",
    "$$ IDF_{word} = \\log\\left(\\frac{total\\_\\#\\_of\\_documents}{\\#\\_of\\_documents\\_containing\\_word}\\right) $$\n",
    "\n",
    "**Vocabulary:**\n",
    "\n",
    "    ['drop', 'help', 'jeff', 'octopus', 'police', 'said', 'sandwich', 'sandwichlessly', 'sobbed', 'stole']\n",
    "\n",
    "**IDF for each word:**\n",
    "\n",
    "    [1.099, 1.099, 1.099, 1.099, 1.099, 1.099, 0.405, 1.099, 1.099, 1.099]\n",
    "\n",
    "### TFIDF\n",
    "\n",
    "**Vocabulary:**\n",
    "\n",
    "    ['drop', 'help', 'jeff', 'octopus', 'police', 'said', 'sandwich', 'sandwichlessly', 'sobbed', 'stole']\n",
    "\n",
    "**TF * IDF:**\n",
    "\n",
    "    ['jeff', 'stole', 'octopus', 'sandwich']\n",
    "    [0, 0, 0.275, 0.275, 0, 0, 0.101, 0, 0, 0.275]\n",
    "\n",
    "    ['help', 'sobbed', 'sandwichlessly']\n",
    "    [0, 0.366, 0, 0, 0, 0, 0, 0.366, 0.366, 0]\n",
    "\n",
    "    ['drop', u'sandwich', 'said', 'sandwich', 'police']\n",
    "    [0.22, 0, 0, 0, 0.22, 0.22, 0.162, 0, 0, 0]\n",
    "\n",
    "Now that we have turned our DOCUMENTS into VECTORS, we can put them into whatever machine learning algorithm we want! \n",
    "We can use whatever kind of similarity measure we please!\n",
    "\n",
    "Wow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "523aa098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.08115802],\n",
       "       [0.08115802, 1.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([[0, 0, 0.275, 0.275, 0, 0, 0.101, 0, 0, 0.275],  [0.22, 0, 0, 0, 0.22, 0.22, 0.162, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b1bdcbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([[0, 0.366, 0, 0, 0, 0, 0, 0.366, 0.366, 0],  [0.22, 0, 0, 0, 0.22, 0.22, 0.162, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a90cb04",
   "metadata": {},
   "source": [
    "### Example with Spam data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb7cc90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#revisit spam ham example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d3a6f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_table('data/SMSSpamCollection', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88d1019c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0                                                  1\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51b3bf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=['spam', 'msg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08c5459c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  spam                                                msg\n",
       "0  ham  Go until jurong point, crazy.. Available only ...\n",
       "1  ham                      Ok lar... Joking wif u oni..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a611480",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_set=set(stopwords)\n",
    "\n",
    "punctuation_set=set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31eb2f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e2f2d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(punctuation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0adcd692",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['msg_cleaned']= df.msg.apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords_set \\\n",
    "                                                   and word not in punctuation_set]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "519de1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go until jurong point, crazy'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str1='Go until jurong point, crazy'.split()\n",
    "' '.join(str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c7d8b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>msg</th>\n",
       "      <th>msg_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>Go jurong point, crazy.. Available bugis n gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  spam                                                msg  \\\n",
       "0  ham  Go until jurong point, crazy.. Available only ...   \n",
       "1  ham                      Ok lar... Joking wif u oni...   \n",
       "\n",
       "                                         msg_cleaned  \n",
       "0  Go jurong point, crazy.. Available bugis n gre...  \n",
       "1                      Ok lar... Joking wif u oni...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ea3991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['msg_cleaned']= df.msg_cleaned.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8bdc5e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>msg</th>\n",
       "      <th>msg_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point, crazy.. available bugis n gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  spam                                                msg  \\\n",
       "0  ham  Go until jurong point, crazy.. Available only ...   \n",
       "1  ham                      Ok lar... Joking wif u oni...   \n",
       "\n",
       "                                         msg_cleaned  \n",
       "0  go jurong point, crazy.. available bugis n gre...  \n",
       "1                      ok lar... joking wif u oni...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a2fc4701",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect= CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b8ee42f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= count_vect.fit_transform(df.msg_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "460ae67a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 8703)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2c1d84ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df.spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4d66ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "132e3896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9827709978463748"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg= LogisticRegression()\n",
    "\n",
    "lg.fit(X_train,y_train)\n",
    "y_pred=lg.predict(X_test)\n",
    "lg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0e724402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1217,    0],\n",
       "       [  24,  152]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "29d940d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'ham', 'ham', ..., 'ham', 'ham', 'ham'], dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d16a71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
