{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "892b81d7",
   "metadata": {},
   "source": [
    "# NLP (Natural language processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "214773c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alwinsolair/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported Modules\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import nltk\n",
    "#new line\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.stem import SnowballStemmer\n",
    "import string\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "# New module is\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline  import Pipeline, FeatureUnion, make_pipeline\n",
    "\n",
    "print(\"Imported Modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fef6ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d93a4d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cbe4b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"Jeff stole my octopus sandwich.\", \n",
    "    \"'Help!' I sobbed, sandwichlessly.\", \n",
    "    \"'Drop the sandwiches!' said the sandwich police.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29f3af1",
   "metadata": {},
   "source": [
    "##Â How do I turn a corpus of documents into a feature matrix?\n",
    "\n",
    "**Words --> numbers?????**\n",
    "\n",
    "**Corpus: list of documents**\n",
    "\n",
    " [\n",
    "     \"Jeff stole my octopus sandwich.\", \n",
    "     \"'Help!' I sobbed, sandwichlessly.\", \n",
    "     \"'Drop the sandwiches!' said the sandwich police.\"\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b7658d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_tokenizer(doc, stops=None, stemmer=None):\n",
    "    doc = word_tokenize(doc.lower())\n",
    "    tokens = [''.join([char for char in tok if char not in string.punctuation]) for tok in doc]\n",
    "    tokens = [tok for tok in tokens if tok]\n",
    "    if stops:\n",
    "        tokens = [tok for tok in tokens if (tok not in stops)]\n",
    "    if stemmer:\n",
    "        tokens = [stemmer.stem(tok) for tok in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97f98974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['jeff', 'stole', 'my', 'octopus', 'sandwich'],\n",
       " ['help', 'i', 'sobbed', 'sandwichlessly'],\n",
       " ['drop', 'the', 'sandwiches', 'said', 'the', 'sandwich', 'police']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_docs = [our_tokenizer(doc) for doc in corpus]\n",
    "tokenized_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c76d92",
   "metadata": {},
   "source": [
    "**Step 1: lowercase, lose punction, split into tokens**\n",
    "\n",
    "    [\n",
    "     ['jeff', 'stole', 'my', 'octopus', 'sandwich'],\n",
    "     ['help', 'i', 'sobbed', 'sandwichlessly'],\n",
    "     ['drop', 'the', 'sandwiches', 'said', 'the', 'sandwich', 'police']\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e70298b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55740eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'i' in stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6ae58ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['jeff', 'stole', 'octopus', 'sandwich'],\n",
       " ['help', 'sobbed', 'sandwichlessly'],\n",
       " ['drop', 'sandwiches', 'said', 'sandwich', 'police']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_docs = [our_tokenizer(doc, stops=stopwords) for doc in corpus]\n",
    "tokenized_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09439cb2",
   "metadata": {},
   "source": [
    "**Step 2: remove stop words**\n",
    "\n",
    "    [\n",
    "     ['jeff', 'stole', 'octopus', 'sandwich'],\n",
    "     ['help', 'sobbed', 'sandwichlessly'],\n",
    "     ['drop', 'sandwiches', 'said', 'sandwich', 'police']\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89072660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['jeff', 'stole', 'octopus', 'sandwich'],\n",
       " ['help', 'sob', 'sandwichless'],\n",
       " ['drop', 'sandwich', 'said', 'sandwich', 'polic']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_docs = [our_tokenizer(doc, stops=stopwords, stemmer=SnowballStemmer('english')) for doc in corpus]\n",
    "tokenized_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb61c89",
   "metadata": {},
   "source": [
    "**Step 3: Stemming/Lemmatization**\n",
    "\n",
    "    [\n",
    "     ['jeff', 'stole', 'octopus', 'sandwich'],\n",
    "     ['help', 'sobbed', 'sandwichlessly'],\n",
    "     ['drop', u'sandwich', 'said', 'sandwich', 'police']\n",
    "    ]\n",
    "\n",
    "**OK now what?**\n",
    "\n",
    "Vocabulary:\n",
    "\n",
    "    ['drop', 'help', 'jeff', 'octopus', 'police', 'said', 'sandwich', 'sandwichlessly', 'sobbed', 'stole']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c8c84e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22caf745",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in tokenized_docs:\n",
    "    vocab_set.update(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fafa6050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['drop', 'help', 'jeff', 'octopus', 'polic', 'said', 'sandwich', 'sandwichless', 'sob', 'stole']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(vocab_set))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801e2ca9",
   "metadata": {},
   "source": [
    "### Section 5.2: Count Vectorizer, TFIDF\n",
    "\n",
    "Count vectorization\n",
    "\n",
    "***Vocabulary:***\n",
    "\n",
    "    ['drop', 'help', 'jeff', 'octopus', 'police', 'said', 'sandwich', 'sandwichlessly', 'sobbed', 'stole']\n",
    "    ['jeff', 'stole', 'octopus', 'sandwich']\n",
    "    [0, 0, 1, 1, 0, 0, 1, 0, 0, 1]\n",
    "\n",
    "    ['help', 'sobbed', 'sandwichlessly']\n",
    "    [0, 1, 0, 0, 0, 0, 0, 1, 1, 0]\n",
    "\n",
    "    ['drop', u'sandwich', 'said', 'sandwich', 'police']\n",
    "    [1, 0, 0, 0, 1, 1, 2, 0, 0, 0]\n",
    "    \n",
    "**Term frequency**\n",
    "\n",
    "$$TF_{word,document} = \\frac{\\#\\_of\\_times\\_word\\_appears\\_in\\_document}{total\\_\\#\\_of\\_words\\_in\\_document}$$\n",
    "\n",
    "    ['jeff', 'stole', 'octopus', 'sandwich']\n",
    "    [0, 0, 1/4, 1/4, 0, 0, 1/4, 0, 0, 1/4]\n",
    "\n",
    "    ['help', 'sobbed', 'sandwichlessly']\n",
    "    [0, 1/3, 0, 0, 0, 0, 0, 1/3, 1/3, 0]\n",
    "\n",
    "    ['drop', u'sandwich', 'said', 'sandwich', 'police']\n",
    "    [1/5, 0, 0, 0, 1/5, 1/5, 2/5, 0, 0, 0]\n",
    "\n",
    "### Document frequency\n",
    "\n",
    "$$ DF_{word} = \\frac{\\#\\_of\\_documents\\_containing\\_word}{total\\_\\#\\_of\\_documents} $$\n",
    "Vocabulary:\n",
    "\n",
    "    ['drop', 'help', 'jeff', 'octopus', 'police', 'said', 'sandwich', 'sandwichlessly', 'sobbed', 'stole']\n",
    "\n",
    "**Document frequency for each word:**\n",
    "\n",
    "    [1/3, 1/3, 1/3, 1/3, 1/3, 1/3, 2/3, 1/3, 1/3, 1/3]\n",
    "\n",
    "### Inverse document frequency\n",
    "\n",
    "$$ IDF_{word} = \\log\\left(\\frac{total\\_\\#\\_of\\_documents}{\\#\\_of\\_documents\\_containing\\_word}\\right) $$\n",
    "\n",
    "**Vocabulary:**\n",
    "\n",
    "    ['drop', 'help', 'jeff', 'octopus', 'police', 'said', 'sandwich', 'sandwichlessly', 'sobbed', 'stole']\n",
    "\n",
    "**IDF for each word:**\n",
    "\n",
    "    [1.099, 1.099, 1.099, 1.099, 1.099, 1.099, 0.405, 1.099, 1.099, 1.099]\n",
    "\n",
    "### TFIDF\n",
    "\n",
    "**Vocabulary:**\n",
    "\n",
    "    ['drop', 'help', 'jeff', 'octopus', 'police', 'said', 'sandwich', 'sandwichlessly', 'sobbed', 'stole']\n",
    "\n",
    "**TF * IDF:**\n",
    "\n",
    "    ['jeff', 'stole', 'octopus', 'sandwich']\n",
    "    [0, 0, 0.275, 0.275, 0, 0, 0.101, 0, 0, 0.275]\n",
    "\n",
    "    ['help', 'sobbed', 'sandwichlessly']\n",
    "    [0, 0.366, 0, 0, 0, 0, 0, 0.366, 0.366, 0]\n",
    "\n",
    "    ['drop', u'sandwich', 'said', 'sandwich', 'police']\n",
    "    [0.22, 0, 0, 0, 0.22, 0.22, 0.162, 0, 0, 0]\n",
    "\n",
    "Now that we have turned our DOCUMENTS into VECTORS, we can put them into whatever machine learning algorithm we want! \n",
    "We can use whatever kind of similarity measure we please!\n",
    "\n",
    "Wow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23d21d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.08115802],\n",
       "       [0.08115802, 1.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([[0, 0, 0.275, 0.275, 0, 0, 0.101, 0, 0, 0.275],  [0.22, 0, 0, 0, 0.22, 0.22, 0.162, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8042c26d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([[0, 0.366, 0, 0, 0, 0, 0, 0.366, 0.366, 0],  [0.22, 0, 0, 0, 0.22, 0.22, 0.162, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b84fe6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
